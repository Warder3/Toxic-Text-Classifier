# Toxic-Text-Classifier
This project can classify whether or not a piece of text is toxic. I would like to preface that I do not condone any of these words, and this project is for educational purposes only.

![](#images/toxic.png)

# Introduction
This project's aim was to build a piece of software that could catch if a piece of text is toxic. Then it will flag the company, so they can take action against the offender. This will discourage people from being toxic. This was also a training project where I got familiar with tensorflow, and improved my python skills.

# Table of Contents
* [Importing the data](#importing-the-data)
* [Cleaning the data](#cleaning-the-data)
* [Exploring the data](#exploring-the-data)
* [Processing data for the model](#processing-data-for-the-model)
* [Embedding](#embedding)
* [Building our model](#building-our-actual-model)
* [Saving our model](#saving-our-model)
* [Writing functions to use program](#Writing-functions-so-we-can-use-model-to-predict-toxicity)

# Technologies
- Python 3.6
- Andaconda Navigator
- Jupyter Notebooks

# Project Status 
Complete

#
